---
title: get technical 
subtitle: more on tensors, modules, and autograd 
---

*(... all while coding a neural network from scratch!)*

In this five-part tutorial, we dig deeper into how torch works, all while playing around with a neural network from the outset.

We start by coding up that network, using nothing but functionality inherent in torch's basic data structure: *tensors*.

Tensors being that important, we then dedicate a whole session to tensors. You'll learn how to create them, convert them to R, juggle around their shapes, and more.

In the remaining three tutorials, we return to the network, and successively get rid of all low-level operations. Increasingly, we let torch do all the hard work for us:

-   first, by letting it do automatic differentiation;

-   second, by using neural network modules, instead of bare tensors; and

-   third, by calling torch optimizers to update the model's weights.

Once you're done, you're more than ready to go on and explore some of the [applied examples](/using_torch)!
