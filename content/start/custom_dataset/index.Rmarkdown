---
title: "Create your own Dataset"
weight: 3
description: | 
  Create a custom torch dataset.
---

Unless the data you're working with comes with some package in the `torch` ecosystem, you'll need to wrap in a `Dataset`.

# `torch` `Dataset` objects

A `Dataset` is an R6 object that knows how to iterate over data. This is because it acts as supplier to a `DataLoader` , who will ask it to return some number of items. How many? That is dependent on the batch size -- but batch sizes are handled by `DataLoaders`, so it needn't be concerned about that. All it has to know is what to do when asked for, e.g., item no. 7.

While a `Dataset` may have any number of methods -- each responsible for some aspect of pre-processing logic, for example -- just three methods are required:

-   `initialize()` , to pre-process and store the data;

-   `.getitem(i)`, to pick the item at position `i`, and

-   `.length()`, to indicate to the `DataLoader` how many items it has.

Let's see an example.

# Penguins

`penguins` is a very nice dataset that lives in the `palmerpenguins` CRAN package.

```{r}
library(palmerpenguins)

penguins
```

There are three species, and we'll predict them from all available information: "biometrics" like `bill_length_mm`, geographic indicators like the `island` they live on, and more. The predictors are of two different types, categorical and continuous.

Continuous features, of R type `double`, may be fed to `torch` without further ado. We just directly use them to initialize a `torch` tensor, which will be of type `Float`:

```{r}
torch_tensor(1)
```

    torch_tensor
     1
    [ CPUFloatType{1} ]

It's different with categorical data though. Firstly, `torch` needs all data to be in numerical form, so vectors of type `character` need to become factors -- which can then be treated as numeric via level extraction. In the `penguins` dataset, `island`, `sex` , as well as the target column, `species`, are factors already. So can we just do an `as.numeric()` and that's it?

Not quite: We also need to reflect on the semantic side of things.

# Categorical data in deep learning

If we just replace islands *Biscoe*, *Dream*, and *Torgersen* by numbers 1, 2, and 3, we'd present them to the network as interval data, which of course they're not. We have two options: transform them to one-hot vectors, where e.g. *Biscoe* would be `0,0,1`, *Dream* `0,1,0`, and *Torgersen*, `1,0,0`, or leave them as they are, but have the network map each discrete value to a multidimensional, continuous representations. The latter is called embedding, and it often helps networks make sense of discrete data.

Embedding modules expect their inputs to be of type `Long`. A tensor created from an R value will have the correct type if make sure it's an `integer`:

```{r}
torch_tensor(as.integer(as.numeric(as.factor("one"))))
```

```{r}
torch_tensor
 1
[ CPULongType{1} ]
```

Now, let's create a dataset for penguins.

# A dataset for penguins

In `initialize()`, we convert the data as planned and store them for later delivery. Like the categorical input features, `species`, the target, is discrete, and thus, converted to `torch` `Long`.

```{r}
library(torch)
penguins_dataset <- dataset(
  
  name = "penguins_dataset",
  
  initialize = function(df) {
    
    df <- na.omit(df) 
    
    # continuous input data (x_cont)   
    x_cont <- df[ , c("bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g", "year")] %>%
      as.matrix()
    self$x_cont <- torch_tensor(x_cont)
    
    # categorical input data (x_cat)
    x_cat <- df[ , c("island", "sex")]
    x_cat$island <- as.integer(x_cat$island)
    x_cat$sex <- as.integer(x_cat$sex)
    self$x_cat <- as.matrix(x_cat) %>% torch_tensor()

    # target data (y)
    species <- as.integer(df$species)
    self$y <- torch_tensor(species)
    
  },
  
  .getitem = function(i) {
     list(self$x_cont[i, ], self$x_cat[i, ], self$y[i])
    
  },
  
  .length = function() {
    self$y$size()[[1]]
  }
 
)
```

Unlike `initialize`, `.getitem(i)` and `.length()` are just one-liners.

Let's see if this behaves like we want it to. We randomly split the data into training and validation sets and query their respective lengths:

```{r}
train_indices <- sample(1:nrow(penguins), 250)

train_ds <- penguins_dataset(penguins[train_indices, ])
valid_ds <- penguins_dataset(penguins[setdiff(1:nrow(penguins), train_indices), ])

length(train_ds)
length(valid_ds)
```

We can index into `Dataset`s in an R-like way:

```{r}
train_ds[1]
```

    [1]]
    torch_tensor
       45.1000
       14.5000
      215.0000
     5000.0000
     2007.0000
    [ CPUFloatType{5} ]

    [[2]]
    torch_tensor
     1
     1
    [ CPULongType{2} ]

    [[3]]
    torch_tensor
    3
    [ CPULongType{} ]

From here on, everything proceeds like in the first tutorial: We use the `Dataset`s to instantiate `DataLoader`s...

```{r}
train_dl <- train_ds %>% dataloader(batch_size = 16, shuffle = TRUE)

valid_dl <- valid_ds %>% dataloader(batch_size = 16, shuffle = FALSE)
```

... and then, create and train the network. The network will look pretty different now though: most notably, you'll see embeddings at work.

# Classifying penguins -- the network  

# Model training
